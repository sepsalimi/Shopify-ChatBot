{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6e01be1",
   "metadata": {},
   "source": [
    "## Why search is better than fine-tuning\n",
    "\n",
    "GPT can learn knowledge in two ways:\n",
    "\n",
    "- Via model weights (i.e., fine-tune the model on a training set)\n",
    "- Via model inputs (i.e., insert the knowledge into an input message)\n",
    "\n",
    "Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n",
    "\n",
    "As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n",
    "\n",
    "In contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n",
    "\n",
    "One downside of text search relative to fine-tuning is that each model is limited by a maximum amount of text it can read at once:\n",
    "\n",
    "| Model           | Maximum text length       |\n",
    "|-----------------|---------------------------|\n",
    "| `gpt-3.5-turbo` | 4,096 tokens (~5 pages)   |\n",
    "| `gpt-4`         | 8,192 tokens (~10 pages)  |\n",
    "| `gpt-4-32k`     | 32,768 tokens (~40 pages) |\n",
    "\n",
    "Continuing the analogy, you can think of the model like a student who can only look at a few pages of notes at a time, despite potentially having shelves of textbooks to draw upon.\n",
    "\n",
    "Therefore, to build a system capable of drawing upon large quantities of text to answer questions, we recommend using a Search-Ask approach.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78fba1de",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Text can be searched in many ways. E.g.,\n",
    "\n",
    "- Lexical-based search\n",
    "- Graph-based search\n",
    "- Embedding-based search\n",
    "\n",
    "This example notebook uses embedding-based search. [Embeddings](https://platform.openai.com/docs/guides/embeddings) are simple to implement and work especially well with questions, as questions often don't lexically overlap with their answers.\n",
    "\n",
    "Consider embeddings-only search as a starting point for your own system. Better search systems might combine multiple search methods, along with features like popularity, recency, user history, redundancy with prior search results, click rate data, etc. Q&A retrieval performance may also be improved with techniques like [HyDE](https://arxiv.org/abs/2212.10496), in which questions are first transformed into hypothetical answers before being embedded. Similarly, GPT can also potentially improve search results by automatically transforming questions into sets of keywords or search terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4ca8276-e829-4cff-8905-47534e4b4d4e",
   "metadata": {},
   "source": [
    "## Full procedure\n",
    "\n",
    "Specifically, this notebook demonstrates the following procedure:\n",
    "\n",
    "1. Prepare search data (once per document)\n",
    "    1. Collect: We'll download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "    2. Chunk: Documents are split into short, mostly self-contained sections to be embedded\n",
    "    3. Embed: Each section is embedded with the OpenAI API\n",
    "    4. Store: Embeddings are saved (for large datasets, use a vector database)\n",
    "2. Search (once per query)\n",
    "    1. Given a user question, generate an embedding for the query from the OpenAI API\n",
    "    2. Using the embeddings, rank the text sections by relevance to the query\n",
    "3. Ask (once per query)\n",
    "    1. Insert the question and the most relevant sections into a message to GPT\n",
    "    2. Return GPT's answer\n",
    "\n",
    "### Costs\n",
    "\n",
    "Because GPT is more expensive than embeddings search, a system with a decent volume of queries will have its costs dominated by step 3.\n",
    "\n",
    "- For `gpt-3.5-turbo` using ~1,000 tokens per query, it costs ~$0.002 per query, or ~500 queries per dollar (as of Apr 2023)\n",
    "- For `gpt-4`, again assuming ~1,000 tokens per query, it costs ~$0.03 per query, or ~30 queries per dollar (as of Apr 2023)\n",
    "\n",
    "Of course, exact costs will depend on the system specifics and usage patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ebd41d8",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "We'll begin by:\n",
    "- Importing the necessary libraries\n",
    "- Selecting models for embeddings search and question answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3839a6-9146-4f60-b74b-19abbc24278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "from transformers import GPT2Tokenizer # for counting tokens\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eac70652",
   "metadata": {},
   "source": [
    "### Setting API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffc1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the file is in your script's directory or provide the full path\n",
    "filename = \"N:\\CAREER\\SEPEHR\\EDUCATION\\Brainstation\\Data Science\\GPT API Keys\\OpenAI API.txt\"\n",
    "\n",
    "# Open the file in read mode ('r')\n",
    "with open(filename, 'r') as file:\n",
    "    openai.api_key = str(file.read().strip())  # read the key and strip leading/trailing white spaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1fd9115",
   "metadata": {},
   "source": [
    "#### For Lists of Models available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbaf51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ada',\n",
      " 'ada-code-search-code',\n",
      " 'ada-code-search-text',\n",
      " 'ada-search-document',\n",
      " 'ada-search-query',\n",
      " 'ada-similarity',\n",
      " 'ada:2020-05-03',\n",
      " 'babbage',\n",
      " 'babbage-code-search-code',\n",
      " 'babbage-code-search-text',\n",
      " 'babbage-search-document',\n",
      " 'babbage-search-query',\n",
      " 'babbage-similarity',\n",
      " 'babbage:2020-05-03',\n",
      " 'code-davinci-edit-001',\n",
      " 'code-search-ada-code-001',\n",
      " 'code-search-ada-text-001',\n",
      " 'code-search-babbage-code-001',\n",
      " 'code-search-babbage-text-001',\n",
      " 'curie',\n",
      " 'curie-instruct-beta',\n",
      " 'curie-search-document',\n",
      " 'curie-search-query',\n",
      " 'curie-similarity',\n",
      " 'curie:2020-05-03',\n",
      " 'cushman:2020-05-03',\n",
      " 'davinci',\n",
      " 'davinci-if:3.0.0',\n",
      " 'davinci-instruct-beta',\n",
      " 'davinci-instruct-beta:2.0.0',\n",
      " 'davinci-search-document',\n",
      " 'davinci-search-query',\n",
      " 'davinci-similarity',\n",
      " 'davinci:2020-05-03',\n",
      " 'gpt-3.5-turbo',\n",
      " 'gpt-3.5-turbo-0301',\n",
      " 'if-curie-v2',\n",
      " 'if-davinci-v2',\n",
      " 'if-davinci:3.0.0',\n",
      " 'text-ada-001',\n",
      " 'text-ada:001',\n",
      " 'text-babbage-001',\n",
      " 'text-babbage:001',\n",
      " 'text-curie-001',\n",
      " 'text-curie:001',\n",
      " 'text-davinci-001',\n",
      " 'text-davinci-002',\n",
      " 'text-davinci-003',\n",
      " 'text-davinci-edit-001',\n",
      " 'text-davinci:001',\n",
      " 'text-embedding-ada-002',\n",
      " 'text-search-ada-doc-001',\n",
      " 'text-search-ada-query-001',\n",
      " 'text-search-babbage-doc-001',\n",
      " 'text-search-babbage-query-001',\n",
      " 'text-search-curie-doc-001',\n",
      " 'text-search-curie-query-001',\n",
      " 'text-search-davinci-doc-001',\n",
      " 'text-search-davinci-query-001',\n",
      " 'text-similarity-ada-001',\n",
      " 'text-similarity-babbage-001',\n",
      " 'text-similarity-curie-001',\n",
      " 'text-similarity-davinci-001',\n",
      " 'whisper-1']\n"
     ]
    }
   ],
   "source": [
    "# Author: Viet Dac Lai\n",
    "import pprint\n",
    "\n",
    "GPT4 = 'gpt-4-0314'\n",
    "MODEL_NAME = GPT4\n",
    "model = openai.Model(MODEL_NAME)\n",
    "\n",
    "def list_all_models():\n",
    "    model_list = openai.Model.list()['data']\n",
    "    model_ids = [x['id'] for x in model_list]\n",
    "    model_ids.sort()\n",
    "    pprint.pprint(model_ids)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    list_all_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fcace0f",
   "metadata": {},
   "source": [
    "#### Troubleshooting: Installing libraries\n",
    "\n",
    "If you need to install any of the libraries above, run `pip install {library_name}` in your terminal.\n",
    "\n",
    "For example, to install the `openai` library, run:\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai` or `%pip install openai`.)\n",
    "\n",
    "After installing, restart the notebook kernel so the libraries can be loaded.\n",
    "\n",
    "#### Troubleshooting: Setting your API key\n",
    "\n",
    "The OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, you can set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9312f62f-e208-4030-a648-71ad97aee74f",
   "metadata": {},
   "source": [
    "### Motivating example: GPT cannot answer questions about current events\n",
    "\n",
    "Because the training data for `gpt-3.5-turbo` and `gpt-4` mostly ends in September 2021, the models cannot answer questions about more recent events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc2d8de",
   "metadata": {},
   "source": [
    "## 1. Prepare search data\n",
    "\n",
    "To save you the time & expense, we've prepared a pre-embedded dataset of a few hundred Wikipedia articles about the 2022 Winter Olympics.\n",
    "\n",
    "To see how we constructed this dataset, or to modify it yourself, see [Embedding Wikipedia articles for search](Embedding_Wikipedia_articles_for_search.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d50792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>App_Name: Product Passage, Developer: Tezisto ...</td>\n",
       "      <td>[-0.012677876278758049, 0.019527003169059753, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>App_Name: Smart Bundles, Developer: Gravitate,...</td>\n",
       "      <td>[0.002880027750506997, 0.015666238963603973, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>App_Name: AnyAsset ‑ Digital Downloads, Develo...</td>\n",
       "      <td>[-0.04260732978582382, -0.0012113063130527735,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  App_Name: Product Passage, Developer: Tezisto ...   \n",
       "1  App_Name: Smart Bundles, Developer: Gravitate,...   \n",
       "2  App_Name: AnyAsset ‑ Digital Downloads, Develo...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.012677876278758049, 0.019527003169059753, ...  \n",
       "1  [0.002880027750506997, 0.015666238963603973, 0...  \n",
       "2  [-0.04260732978582382, -0.0012113063130527735,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path\n",
    "embeddings_path = \"N:\\CAREER\\SEPEHR\\EDUCATION\\Brainstation\\Data Science\\Deliverables\\Hackathon\\OpenAI API\\Embedding Search-Ask\\df_GPT_embedded_final_no_reviews.csv\"\n",
    "\n",
    "# Reading\n",
    "df = pd.read_csv(embeddings_path)\n",
    "\n",
    "# Display\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec1c344c",
   "metadata": {},
   "source": [
    "## 2. Search\n",
    "\n",
    "Now we'll define a search function that:\n",
    "- Takes a user query and a dataframe with text & embedding columns\n",
    "- Embeds the user query with the OpenAI API\n",
    "- Uses distance between query embedding and text embeddings to rank the texts\n",
    "- Returns two lists:\n",
    "    - The top N texts, ranked by relevance\n",
    "    - Their corresponding relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a8c713-c8a9-47dc-85a4-871ee1395566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0efa0f6-4469-457a-89a4-a2f5736a01e0",
   "metadata": {},
   "source": [
    "## 3. Ask\n",
    "\n",
    "With the search function above, we can now automatically retrieve relevant knowledge and insert it into messages to GPT.\n",
    "\n",
    "Below, we define a function `ask` that:\n",
    "- Takes a user query\n",
    "- Searches for text relevant to the query\n",
    "- Stuffs that text into a message for GPT\n",
    "- Sends the message to GPT\n",
    "- Returns GPT's answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "513aeaea",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb18e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Set up tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    return len([token for token in nlp(text)])\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from the DataFrame.\"\"\"\n",
    "    \n",
    "    introduction = \"- Assist the merchant with building their Shopify website. \\\n",
    "        - If you are asked a question, Make sure to ask a question back to make sure you are replying accurately. \\\n",
    "        - If the user asks for recommendations, use the appstore data provided to give 2 recommendations, and write a short summary of each recommendation. \\\n",
    "        - Do not forget to give a recommendation, as you don't want to waste the user's time. \\\n",
    "        - Prioritize free apps.\\\n",
    "        - Put the FULL app name into quotes! We want to be able to query it\\\n",
    "        - Keep it between 300 characters\"\n",
    "\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    \n",
    "    if df is not None:\n",
    "        for _, row in df.iterrows():\n",
    "            text = row['text']\n",
    "            next_article = f'\\n\\nText:\\n\"\"\"\\n{text}\\n\"\"\"'\n",
    "            if (\n",
    "                count_tokens(message + next_article + question)\n",
    "                > token_budget\n",
    "            ):\n",
    "                break\n",
    "            else:\n",
    "                message += next_article\n",
    "    return message + question\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    model_selected = \"gpt-3.5-turbo\",\n",
    "    token_budget: int = 4096-100,\n",
    "    print_message: bool = False,\n",
    "    history: list = []\n",
    "    ) -> str:\n",
    "\n",
    "    \"\"\"Answers query using GPT and dataframe of relevant text and embeddings provided. Recommend 3 apps if conversation is about apps\"\"\"  \n",
    "    message = query\n",
    "      \n",
    "    if print_message:\n",
    "        print(message)\n",
    "\n",
    "    messages = history.copy()  # Make sure to copy the history, so it doesn't get modified\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    while count_tokens(''.join([m['content'] for m in messages])) > token_budget:\n",
    "        messages.pop(1)  # Remove the oldest messages first (pop(0) is the system message)\n",
    "\n",
    "    max_tokens = token_budget - count_tokens(''.join([m['content'] for m in messages]))  # Remaining tokens for response\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_selected,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Store the new system message to history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_message})\n",
    "\n",
    "    return response_message\n",
    "\n",
    "# Initialise the conversation history\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that aids in Shopify Webstore Development by suggesting apps and providing captions for products.\"},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f2b0927",
   "metadata": {},
   "source": [
    "### Example questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1400158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are 6 great starting apps for different purposes:\n",
      "\n",
      "1. Trello - Trello is a project management app that allows you to organize tasks and projects on a visual board. It's great for keeping track of to-do lists, deadlines, and team collaboration.\n",
      "\n",
      "2. Grammarly - Grammarly is a writing app that helps you improve your writing skills by checking for grammar, spelling, and punctuation errors. It's great for writing emails, reports, and other business documents.\n",
      "\n",
      "3. Canva - Canva is a graphic design app that allows you to create professional-looking designs for social media, marketing materials, and presentations. It's great for creating visual content without needing to hire a graphic designer.\n",
      "\n",
      "4. Google Analytics - Google Analytics is a web analytics app that allows you to track website traffic, user behavior, and conversion rates. It's great for understanding how your website is performing and making data-driven decisions.\n",
      "\n",
      "5. Slack - Slack is a communication app that allows you to communicate with your team in real-time. It's great for team collaboration, sharing files, and keeping everyone on the same page.\n",
      "\n",
      "6. Hootsuite - Hootsuite is a social media management app that allows you to manage multiple social media accounts in one place. It's great for scheduling posts, monitoring social media activity, and analyzing social media performance.\n"
     ]
    }
   ],
   "source": [
    "# Ask the first question\n",
    "response = ask('Recommend me 6 good starting apps', model_selected=GPT_MODEL, history=history)  \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0083666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are several alternatives to Grammarly that you can consider:\n",
      "\n",
      "1. ProWritingAid - ProWritingAid is a writing app that offers grammar and spelling checks, as well as style and readability suggestions. It also offers a thesaurus and a contextual thesaurus to help you find the right words.\n",
      "\n",
      "2. Hemingway Editor - Hemingway Editor is a writing app that helps you simplify your writing and make it more readable. It highlights complex sentences, adverbs, and passive voice, and suggests simpler alternatives.\n",
      "\n",
      "3. Ginger Software - Ginger Software is a writing app that offers grammar and spelling checks, as well as a sentence rephraser and a translator. It also offers a personal trainer feature that helps you improve your writing skills over time.\n",
      "\n",
      "4. WhiteSmoke - WhiteSmoke is a writing app that offers grammar and spelling checks, as well as style and punctuation suggestions. It also offers a translator and a plagiarism checker.\n",
      "\n",
      "5. LanguageTool - LanguageTool is a writing app that offers grammar and spelling checks in multiple languages. It also offers style and punctuation suggestions, as well as a thesaurus and a translator.\n"
     ]
    }
   ],
   "source": [
    "# Ask the second question\n",
    "response = ask('Are there any alternatives to the second one?', model_selected=GPT_MODEL, history=history)  \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d4e0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the alternatives I mentioned, LanguageTool is the cheapest option as it offers a free version with basic grammar and spelling checks. However, the free version has limited features and is only available for personal use. \n",
      "\n",
      "ProWritingAid also offers a free version with limited features, but it has a word limit of 500 words per check. The premium version of ProWritingAid starts at $70 per year.\n",
      "\n",
      "Hemingway Editor has a one-time fee of $19.99 for the desktop app, and the online version is free to use.\n",
      "\n",
      "Ginger Software offers a free version with basic grammar and spelling checks, but it has limited features. The premium version of Ginger Software starts at $20.97 per month.\n",
      "\n",
      "WhiteSmoke offers a free trial, but the premium version starts at $5 per month for the basic plan and goes up to $11.50 per month for the premium plan.\n"
     ]
    }
   ],
   "source": [
    "# Ask the third question\n",
    "response = ask('Which of these are cheapest?', model_selected=GPT_MODEL, history=history)  \n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83a30782",
   "metadata": {},
   "source": [
    "#### Implementing .py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a352330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1748 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to help! Can you please clarify what you mean by \"starting apps\"? Are you looking for apps to help you get started with building your Shopify website, or are you looking for apps to help you with specific tasks such as marketing, inventory management, or customer service?\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the class\n",
    "assistant = ShopifyAssistant()\n",
    "\n",
    "# Call the ask() method with a query\n",
    "response = assistant.ask(query=\"Recommend me 6 good starting apps\")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8978290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1752 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! What kind of products will you be selling on your website?\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the class\n",
    "assistant = ShopifyAssistant()\n",
    "\n",
    "# Call the ask() method with a query\n",
    "response = assistant.ask(query=\"Yes I'm looking to build my shopify website\")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f83757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1744 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of jewelry are you looking to sell on your Shopify website?\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the class\n",
    "assistant = ShopifyAssistant()\n",
    "\n",
    "# Call the ask() method with a query\n",
    "response = assistant.ask(query=\"Jewelry\")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7da5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
